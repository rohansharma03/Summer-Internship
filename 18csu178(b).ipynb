{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import h5py;\n",
    "import matplotlib.pyplot as plt;\n",
    "from testCases_v3 import *;\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward;\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0); # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest';\n",
    "plt.rcParams['image.cmap'] = 'gray';\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_l = np.load('C:\\\\Users\\\\Dell\\\\NCU\\\\Machine Learning\\\\X.npy')\n",
    "Y_l = np.load('C:\\\\Users\\\\Dell\\\\NCU\\\\Machine Learning\\\\Y.npy')\n",
    "img_size = 64\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x_l[260].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(x_l[900].reshape(img_size, img_size))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    " \"\"\"\n",
    " Implements the sigmoid activation in numpy\n",
    " Arguments:\n",
    " Z -- numpy array of any shape\n",
    " Returns:\n",
    " A -- output of sigmoid(z), same shape as Z\n",
    " cache -- returns Z as well, useful during backpropagation\n",
    " \"\"\"\n",
    " A = 1 / (1 + np.exp(-Z));\n",
    " cache = Z;\n",
    " return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    " \"\"\"\n",
    " Implement the backward propagation for a single SIGMOID unit.\n",
    " Arguments:\n",
    " dA -- post-activation gradient, of any shape\n",
    " cache -- 'Z' where we store for computing backward propagation efficiently\n",
    " Returns:\n",
    " dZ -- Gradient of the cost with respect to Z\n",
    " \"\"\"\n",
    " Z = cache;\n",
    " s = 1 / (1 + np.exp(-Z));\n",
    " dZ = dA * s * (1 - s);\n",
    " assert (dZ.shape == Z.shape);\n",
    " return dZ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    " \"\"\"\n",
    " Implement the RELU function.\n",
    " Arguments:\n",
    " Z -- Output of the linear layer, of any shape\n",
    " Returns:\n",
    " A -- Post-activation parameter, of the same shape as Z\n",
    " cache -- a python dictionary containing \"A\" ; stored for computing the backward pass effic\n",
    " \"\"\"\n",
    " A = np.maximum(0,Z);\n",
    " assert(A.shape == Z.shape);\n",
    " cache = Z;\n",
    " return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    " \"\"\"\n",
    " Implement the backward propagation for a single RELU unit.\n",
    " Arguments:\n",
    " dA -- post-activation gradient, of any shape\n",
    " cache -- 'Z' where we store for computing backward propagation efficiently\n",
    " Returns:\n",
    " dZ -- Gradient of the cost with respect to Z\n",
    " \"\"\"\n",
    " Z = cache;\n",
    " dZ = np.array(dA, copy = True); # just converting dz to a correct object.\n",
    " # When z <= 0, you should set dz to 0 as well.\n",
    " dZ[Z <= 0] = 0;\n",
    " assert (dZ.shape == Z.shape);\n",
    " return dZ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (410, 64, 64)\n",
      "Y shape:  (410, 1)\n"
     ]
    }
   ],
   "source": [
    "# Join a sequence of arrays along an row axis.\n",
    "X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \n",
    "z = np.zeros(205)\n",
    "o = np.ones(205)\n",
    "Y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\n",
    "print(\"X shape: \" , X.shape)\n",
    "print(\"Y shape: \" , Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then lets create x_train, y_train, x_test, y_test arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train flatten (348, 4096)\n",
      "X test flatten (62, 4096)\n"
     ]
    }
   ],
   "source": [
    "X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\n",
    "print(\"X train flatten\",X_train_flatten.shape)\n",
    "print(\"X test flatten\",X_test_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (4096, 348)\n",
      "x test:  (4096, 62)\n",
      "y train:  (1, 348)\n",
      "y test:  (1, 62)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train_flatten.T\n",
    "x_test = X_test_flatten.T\n",
    "y_train = Y_train.T\n",
    "y_test = Y_test.T\n",
    "print(\"x train: \",x_train.shape)\n",
    "print(\"x test: \",x_test.shape)\n",
    "print(\"y train: \",y_train.shape)\n",
    "print(\"y test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Size and Weights and Bias Initialization\n",
    "In x_train, there are 348 images x^(348)\n",
    "To obtain variety, we should initialize weights randomly; but as small number like 0.01.\n",
    "We should initialize weights as small number since when we use tanh activation function with big value weights, its derivative is approximetly 0 and we cannot generate optimistic result. (ex.: tanh(5) ~= 0.999 and Its derivative is ~ 0).\n",
    "However we can initialize bias as zero.\n",
    "I prefer to use 3 nodes hidden layer when implementing 2 layer ANN. Because of this reason:\n",
    " * Shape of weight1 :  (3, 4096)\n",
    " * Shape of weight2 :  (1, 3)\n",
    " * Shape of bias1 :  (3, 1)\n",
    " * Shape of bias2 :  (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weight1 :  (3, 4096)\n",
      "Shape of weight2 :  (1, 3)\n",
      "Shape of bias1 :  (3, 1)\n",
      "Shape of bias2 :  (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weight1': array([[ 0.16243454, -0.06117564, -0.05281718, ..., -0.14788339,\n",
       "         -0.04929046,  0.07949473],\n",
       "        [-0.10922135,  0.20865146,  0.1316653 , ..., -0.06137821,\n",
       "          0.13543076,  0.05320339],\n",
       "        [-0.06501626,  0.22848077, -0.0540921 , ..., -0.05272142,\n",
       "         -0.03803397,  0.09494124]]), 'bias1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]), 'weight2': array([[ 0.10092311,  0.0229889 , -0.06640991]]), 'bias2': array([[0.]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parameter_initialize(x_train,y_train):\n",
    "    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0])*0.1,\n",
    "                  \"bias1\": np.zeros((3,1)),\n",
    "                  \"weight2\": np.random.randn(y_train.shape[0],3)*0.1,\n",
    "                  \"bias2\" : np.zeros((y_train.shape[0],1))}\n",
    "    \n",
    "    print(\"Shape of weight1 : \",parameters[\"weight1\"].shape)\n",
    "    print(\"Shape of weight2 : \",parameters[\"weight2\"].shape)\n",
    "    print(\"Shape of bias1 : \",parameters[\"bias1\"].shape)\n",
    "    print(\"Shape of bias2 : \",parameters[\"bias2\"].shape)\n",
    "\n",
    "    return parameters\n",
    "parameter_initialize(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation\n",
    "Its very similar with logistic regrassion.\n",
    "In this part we use two different activation functions which are sigmoid and tanh.\n",
    "Firstly, we should initialize sigmoid function and for tanh function we use numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    A = 1/(1+np.exp(-z))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x_train,parameters):\n",
    "    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    results = {\"Z1\": Z1, \"A1\":A1,\"Z2\":Z2,\"A2\":A2}\n",
    "    return A2, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A2,Y):\n",
    "    logaritmic_probability = np.multiply(np.log(A2),Y)\n",
    "    cost = -np.sum(logaritmic_probability)/Y.shape[1]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Propagation <BR>\n",
    " In this part, we take derivative of weights, for weight1 and weight2, bais, for bais1 and bais2, results, for Z1 and Z2 according to cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters,results,X,Y):\n",
    "    dZ2 = results[\"A2\"]-Y\n",
    "    dW2 = np.dot(dZ2,results[\"A1\"].T)/X.shape[1]\n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True)/X.shape[1]\n",
    "    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1-np.power(results[\"A1\"],2))\n",
    "    dW1 = np.dot(dZ1,X.T)/X.shape[1]\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims = True)/X.shape[1]\n",
    "    gradients = {\"dweight1\": dW1,\n",
    "                \"dweight2\": dW2,\n",
    "                \"dbias1\": db1,\n",
    "                \"dbias2\":db2}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prameters(parameters,grand,learning_rate = 0.01):\n",
    "    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grand[\"dweight1\"],\n",
    "                  \"bias1\": parameters[\"bias1\"]-learning_rate*grand[\"dbias1\"],\n",
    "                  \"weight2\": parameters[\"weight2\"]-learning_rate*grand[\"dweight2\"],\n",
    "                  \"bias2\" : parameters[\"bias2\"]-learning_rate*grand[\"dbias2\"]\n",
    "                 }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(parameters, x_test):\n",
    "    A2, results = forward_propagation(x_test,parameters)\n",
    "    prediction = np.zeros((1,x_test.shape[1]))\n",
    "    \n",
    "    for i in range(A2.shape[1]):\n",
    "        if A2[0,i] <= 0.5:\n",
    "            prediction[0,i] = 0\n",
    "        else:\n",
    "            prediction[0,i] = 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer ANN Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weight1 :  (3, 4096)\n",
      "Shape of weight2 :  (1, 3)\n",
      "Shape of bias1 :  (3, 1)\n",
      "Shape of bias2 :  (1, 1)\n",
      "Cost after iteration 0 0.342632\n",
      "Cost after iteration 100 0.316843\n",
      "Cost after iteration 200 0.278211\n",
      "Cost after iteration 300 0.222048\n",
      "Cost after iteration 400 0.171436\n",
      "Cost after iteration 500 0.135530\n",
      "Cost after iteration 600 0.111432\n",
      "Cost after iteration 700 0.094889\n",
      "Cost after iteration 800 0.082982\n",
      "Cost after iteration 900 0.073971\n",
      "Cost after iteration 1000 0.066836\n",
      "Cost after iteration 1100 0.060947\n",
      "Cost after iteration 1200 0.055879\n",
      "Cost after iteration 1300 0.051316\n",
      "Cost after iteration 1400 0.047013\n",
      "Cost after iteration 1500 0.042827\n",
      "Cost after iteration 1600 0.038785\n",
      "Cost after iteration 1700 0.034991\n",
      "Cost after iteration 1800 0.031447\n",
      "Cost after iteration 1900 0.028107\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.42528735632185 %\n",
      "test accuracy: 96.7741935483871 %\n"
     ]
    }
   ],
   "source": [
    "def two_layer_ANN_model(x_train, y_train, x_test, y_test, number_of_iteration):\n",
    "    cost_list = []\n",
    "    index = []\n",
    "    parameters = parameter_initialize(x_train,y_train)\n",
    "    for i in range(number_of_iteration):\n",
    "        A2, results = forward_propagation(x_train,parameters)\n",
    "        cost_result = cost(A2,y_train)\n",
    "        gradients = backward_propagation(parameters,results,x_train,y_train)\n",
    "        parameters = update_prameters(parameters, gradients)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost_result)\n",
    "            index.append(i)\n",
    "            print(\"Cost after iteration %i %f\" %(i,cost_result))\n",
    "    plt.plot(index,cost_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of ıteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    y_prediction_test = prediction(parameters,x_test)\n",
    "    y_prediction_train = prediction(parameters,x_train)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    return parameters\n",
    "parameters = two_layer_ANN_model(x_train,y_train,x_test,y_test,2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    " \"\"\"\n",
    " Implements the sigmoid activation in numpy\n",
    " Arguments:\n",
    " Z -- numpy array of any shape\n",
    " Returns:\n",
    " A -- output of sigmoid(z), same shape as Z\n",
    " cache -- returns Z as well, useful during backpropagation\n",
    " \"\"\"\n",
    " A = 1 / (1 + np.exp(-Z));\n",
    " cache = Z;\n",
    " return A, cache;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Forward propagation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_test_case():\n",
    "    np.random.seed(1);\n",
    "    A=np.random.randn(3,2);\n",
    "    W=np.random.randn(1,3);\n",
    "    b=np.random.randn(1,1);\n",
    "    return A,W,b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, activation_cache = sigmoid(Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, activation_cache = relu(Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward_test_case():\n",
    "    np.random.seed(2)\n",
    "    A_prev = np.random.randn(3,2)\n",
    "    W= np.random.randn(1,3)\n",
    "    b= np.random.randn(1,1)\n",
    "    return A_prev, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l layered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W = parameters['W' + str(L)]\n",
    "    b = parameters['b' + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward_test_case():\n",
    "    np.random.seed(1);\n",
    "    X = np.random.randn(4,2);\n",
    "    W1 = np.random.randn(3,4);\n",
    "    b1 = np.random.randn(3,1);\n",
    "    W2 = np.random.randn(1,3);\n",
    "    b2 = np.random.randn(1,1);\n",
    "    parameters = {\"W1\": W1,\n",
    "    \"b1\": b1,\n",
    "    \"W2\": W2,\n",
    "    \"b2\": b2};\n",
    "    return X, parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test_case():\n",
    "    Y = np.asarray([[1, 1, 1]]);\n",
    "    aL = np.array([[.8,.9,0.4]]);\n",
    "    return Y, aL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    " \"\"\"\n",
    " Implement the backward propagation for a single RELU unit.\n",
    " Arguments:\n",
    " dA -- post-activation gradient, of any shape\n",
    " cache -- 'Z' where we store for computing backward propagation efficiently\n",
    " Returns:\n",
    " dZ -- Gradient of the cost with respect to Z\n",
    " \"\"\"\n",
    " Z = cache;\n",
    " dZ = np.array(dA, copy = True); # just converting dz to a correct object.\n",
    " # When z <= 0, you should set dz to 0 as well.\n",
    " dZ[Z <= 0] = 0;\n",
    " assert (dZ.shape == Z.shape);\n",
    " return dZ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward_test_case():\n",
    "    np.random.seed(1);\n",
    "    dZ = np.random.randn(1,2);\n",
    "    A = np.random.randn(3,2);\n",
    "    W = np.random.randn(1,3);\n",
    "    b = np.random.randn(1,1);\n",
    "    linear_cache = (A, W, b);\n",
    "    return dZ, linear_cache;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case();\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache);\n",
    "print (\"dA_prev = \"+ str(dA_prev));\n",
    "print (\"dW = \" + str(dW));\n",
    "print (\"db = \" + str(db));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward_test_case():\n",
    "    np.random.seed(2);\n",
    "    dA = np.random.randn(1,2);\n",
    "    A = np.random.randn(3,2);\n",
    "    W = np.random.randn(1,3);\n",
    "    b = np.random.randn(1,1);\n",
    "    Z = np.random.randn(1,2);\n",
    "    linear_cache = (A, W, b);\n",
    "    activation_cache = Z;\n",
    "    linear_activation_cache = (linear_cache, activation_cache);\n",
    "    return dA, linear_activation_cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    print(\"L = \"+str(L))\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    print(\"dA\"+ str(L-1)+\" = \"+str(grads[\"dA\" + str(L-1)]))\n",
    "    print(\"dW\"+ str(L)+\" = \"+str(grads[\"dW\" + str(L)]))\n",
    "    print(\"db\"+ str(L)+\" = \"+str(grads[\"db\" + str(L)]))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward_test_case():\n",
    " \"\"\"\n",
    " X = np.random.rand(3,2)\n",
    " Y = np.array([[1, 1]])\n",
    " parameters = {'W1': np.array([[ 1.78862847, 0.43650985, 0.09649747]]), 'b1': np.array([[\n",
    " aL, caches = (np.array([[ 0.60298372, 0.87182628]]), [((np.array([[ 0.20445225, 0.878117\n",
    " [ 0.02738759, 0.67046751],\n",
    " [ 0.4173048 , 0.55868983]]),\n",
    " np.array([[ 1.78862847, 0.43650985, 0.09649747]]),\n",
    " np.array([[ 0.]])),\n",
    " np.array([[ 0.41791293, 1.91720367]]))])\n",
    " \"\"\"\n",
    " np.random.seed(3)\n",
    " AL = np.random.randn(1, 2)\n",
    " Y = np.array([[1, 0]])\n",
    " A1 = np.random.randn(4,2)\n",
    " W1 = np.random.randn(3,4)\n",
    " b1 = np.random.randn(3,1)\n",
    " Z1 = np.random.randn(3,2)\n",
    " linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    " A2 = np.random.randn(3,2)\n",
    " W2 = np.random.randn(1,3)\n",
    " b2 = np.random.randn(1,1)\n",
    " Z2 = np.random.randn(1,2)\n",
    " linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    " caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    " return AL, Y, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = 2\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n",
      "dW2 = [[-0.39202432 -0.13325855 -0.04601089]]\n",
      "db2 = [[0.15187861]]\n",
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_test_case():\n",
    " np.random.seed(2)\n",
    " W1 = np.random.randn(3,4)\n",
    " b1 = np.random.randn(3,1)\n",
    " W2 = np.random.randn(1,3)\n",
    " b2 = np.random.randn(1,1)\n",
    " parameters = {\"W1\": W1,\n",
    " \"b1\": b1,\n",
    " \"W2\": W2,\n",
    " \"b2\": b2}\n",
    " np.random.seed(3)\n",
    " dW1 = np.random.randn(3,4)\n",
    " db1 = np.random.randn(3,1)\n",
    " dW2 = np.random.randn(1,3)\n",
    " db2 = np.random.randn(1,1)\n",
    " grads = {\"dW1\": dW1,\n",
    " \"db1\": db1,\n",
    " \"dW2\": dW2,\n",
    " \"db2\": db2}\n",
    " return parameters, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Conclusion\n",
    "Congrats on implementing all the functions required for building a deep neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
